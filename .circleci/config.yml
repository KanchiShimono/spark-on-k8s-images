version: 2.1

orbs:
  slack: circleci/slack@3.4.2

jobs:
  build:
    machine: true
    steps:
      - checkout

      - run:
          name: Set ENV
          command: |
            echo "export PYTHON_VERSION=3.7.7" >> $BASH_ENV
            echo "export SPARK_VERSION=3.0.0" >> $BASH_ENV
            echo "export HADOOP_VERSION=3.2" >> $BASH_ENV
            echo "export HADOOP_AWS_JAR_VERSION=3.2.0" >> $BASH_ENV
            echo "export AWS_JAVA_SDK_VERSION=1.11.375" >> $BASH_ENV
            echo "export SPARK_UID=185" >> $BASH_ENV
            echo "export IMAGE_TAG=latest" >> $BASH_ENV
            echo "export OWNER=kanchishimono" >> $BASH_ENV

      - run:
          name: Install spark
          working_directory: ~/project/base
          command: |
            wget https://archive.apache.org/dist/spark/spark-${SPARK_VERSION}/spark-${SPARK_VERSION}-bin-hadoop${HADOOP_VERSION}.tgz
            tar xzf spark-${SPARK_VERSION}-bin-hadoop${HADOOP_VERSION}.tgz
            mv spark-${SPARK_VERSION}-bin-hadoop${HADOOP_VERSION} spark

      - run:
          name: Build base Spark Docker image
          working_directory: ~/project/base/spark
          command: |
            bin/docker-image-tool.sh -r ${OWNER} -t base build

      - run:
          name: Build base PySpark Docker image
          working_directory: ~/project/base
          command: >-
            docker build
            -t ${OWNER}/pyspark-base:${IMAGE_TAG}
            --build-arg base=${OWNER}/spark:base
            --build-arg python_version=${PYTHON_VERSION}
            --build-arg spark_uid=${SPARK_UID}
            --build-arg HADOOP_AWS_JAR_VERSION=${HADOOP_AWS_JAR_VERSION}
            --build-arg AWS_JAVA_SDK_VERSION=${AWS_JAVA_SDK_VERSION}
            .

      - run:
          name: Build PySpark master Docker image
          working_directory: ~/project/master
          command: >-
            docker build
            -t ${OWNER}/pyspark-master:${IMAGE_TAG}
            --build-arg base=${OWNER}/pyspark-base:${IMAGE_TAG}
            .

      - run:
          name: Build PySpark worker Docker image
          working_directory: ~/project/worker
          command: >-
            docker build
            -t ${OWNER}/pyspark-worker:${IMAGE_TAG}
            --build-arg base=${OWNER}/pyspark-base:${IMAGE_TAG}
            --build-arg spark_uid=${SPARK_UID}
            .

      - run:
          name: Build history server Docker image
          working_directory: ~/project/history-server
          command: >-
            docker build
            -t ${OWNER}/pyspark-history-server:${IMAGE_TAG}
            --build-arg base=${OWNER}/spark:base
            .

      - run:
          name: Push Docker image to Docker Hub
          command: |
            docker login -u ${OWNER} -p ${DOCKERHUB_PASSWD}
            docker push ${OWNER}/pyspark-base:${IMAGE_TAG}
            docker push ${OWNER}/pyspark-master:${IMAGE_TAG}
            docker push ${OWNER}/pyspark-worker:${IMAGE_TAG}
            docker push ${OWNER}/pyspark-history-server:${IMAGE_TAG}

      - slack/status:
