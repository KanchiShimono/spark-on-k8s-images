version: 2.1

orbs:
  slack: circleci/slack@3.4.2

jobs:
  build:
    machine: true
    steps:
      - checkout

      - run:
          name: Set ENV
          command: |
            echo "export PYTHON_VERSION=3.7.7" >> $BASH_ENV
            echo "export SPARK_VERSION=3.0.0" >> $BASH_ENV
            echo "export HADOOP_VERSION=3.2" >> $BASH_ENV
            echo "export HADOOP_AWS_JAR_VERSION=3.2.0" >> $BASH_ENV
            echo "export AWS_JAVA_SDK_VERSION=1.11.375" >> $BASH_ENV
            echo "export SPARK_UID=185" >> $BASH_ENV
            echo "export IMAGE_TAG=latest" >> $BASH_ENV
            echo "export OWNER=kanchishimono" >> $BASH_ENV

      - run:
          name: Download spark
          working_directory: ~/project/pyspark
          command: |
            wget https://archive.apache.org/dist/spark/spark-${SPARK_VERSION}/spark-${SPARK_VERSION}-bin-hadoop${HADOOP_VERSION}.tgz
            tar xzf spark-${SPARK_VERSION}-bin-hadoop${HADOOP_VERSION}.tgz
            mv spark-${SPARK_VERSION}-bin-hadoop${HADOOP_VERSION} spark

      # Download under the pyspark directory because some files are used on building pyspark image step
      - run:
          name: Build Spark Docker image by official tool
          working_directory: ~/project/pyspark/spark
          command: |
            bin/docker-image-tool.sh -r spark -t ${IMAGE_TAG} build

      - run:
          name: Build custom Spark Docker image for Spark on k8s
          working_directory: ~/project/spark
          command: >-
            docker build
            -t ${OWNER}/spark:${IMAGE_TAG}
            --build-arg base=spark/spark:${IMAGE_TAG}
            --build-arg spark_uid=${SPARK_UID}
            --build-arg HADOOP_AWS_JAR_VERSION=${HADOOP_AWS_JAR_VERSION}
            --build-arg AWS_JAVA_SDK_VERSION=${AWS_JAVA_SDK_VERSION}
            .

      - run:
          name: Build PySpark Docker image
          working_directory: ~/project/pyspark
          command: >-
            docker build
            -t ${OWNER}/pyspark:${IMAGE_TAG}
            --build-arg base=${OWNER}/spark:${IMAGE_TAG}
            --build-arg python_version=${PYTHON_VERSION}
            --build-arg spark_uid=${SPARK_UID}
            .

      - run:
          name: Build PySpark with Jupyter server Docker image
          working_directory: ~/project/pyspark-jupyter
          command: >-
            docker build
            -t ${OWNER}/pyspark-jupyter:${IMAGE_TAG}
            --build-arg base=${OWNER}/pyspark:${IMAGE_TAG}
            --build-arg spark_uid=${SPARK_UID}
            .

      - run:
          name: Push Docker image to Docker Hub
          command: |
            docker login -u ${OWNER} -p ${DOCKERHUB_PASSWD}
            docker push ${OWNER}/spark:${IMAGE_TAG}
            docker push ${OWNER}/pyspark:${IMAGE_TAG}
            docker push ${OWNER}/pyspark-jupyter:${IMAGE_TAG}

      - slack/status
