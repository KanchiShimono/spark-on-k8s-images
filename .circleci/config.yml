version: 2.1

jobs:
  build:
    machine: true
    steps:
      - checkout

      - run:
          name: Set ENV
          command: |
            echo "export PYTHON_VERSION=3.7.7" >> $BASH_ENV
            echo "export SPARK_VERSION=2.4.5" >> $BASH_ENV
            echo "export HADOOP_VERSION=2.7" >> $BASH_ENV
            echo "export IMAGE_TAG=latest" >> $BASH_ENV
            echo "export OWNER=kanchishimono" >> $BASH_ENV

      - run:
          name: Install spark
          command: |
            wget https://archive.apache.org/dist/spark/spark-${SPARK_VERSION}/spark-${SPARK_VERSION}-bin-hadoop${HADOOP_VERSION}.tgz
            tar xzf spark-${SPARK_VERSION}-bin-hadoop${HADOOP_VERSION}.tgz
            ln -s spark-${SPARK_VERSION}-bin-hadoop${HADOOP_VERSION} spark

      - run:
          name: Build base worker Docker image
          command: |
            cd spark
            bin/docker-image-tool.sh -r ${OWNER} -t base build

      - run:
          name: Custom pyspark worker Docker image
          command: |
            docker build -t ${OWNER}/pyspark-worker:${IMAGE_TAG} --build-arg base=${OWNER}/spark:base --build-arg python_version=${PYTHON_VERSION} -f ./worker/Dockerfile

      - run:
          name: Build base master and history server Docker image
          command: |
            cd base
            docker build -t ${OWNER}/pyspark-base:${IMAGE_TAG} --build-arg SPARK_VERSION=${SPARK_VERSION} --build-arg HADOOP_VERSION=${HADOOP_VERSION} --build-arg python_version=${PYTHON_VERSION} .

      - run:
          name: Build master Docker image
          command: |
            cd master
            docker build -t ${OWNER}/pyspark-master:${IMAGE_TAG} --build-arg base=${OWNER}/spark-base:${IMAGE_TAG} .

      - run:
          name: Build history server Docker image
          command: |
            cd history-server
            docker build -t ${OWNER}/pyspark-history-server:${IMAGE_TAG} --build-arg base=${OWNER}/spark-base:${IMAGE_TAG} .

      - run:
          name: Push Docker image to Docker Hub
          command: |
            cd spark
            docker login -u ${OWNER} -p ${DOCKERHUB_PASSWD}
            docker push ${OWNER}/pyspark-base:${IMAGE_TAG}
            docker push ${OWNER}/pyspark-master:${IMAGE_TAG}
            docker push ${OWNER}/pyspark-worker:${IMAGE_TAG}
            docker push ${OWNER}/pyspark-history-server:${IMAGE_TAG}
